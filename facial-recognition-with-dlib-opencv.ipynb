{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798865bc",
   "metadata": {},
   "source": [
    "# Simple Facial Recognition Using Dlib And OpenCv\n",
    "The facial recognition system is a computer vision application that can detect and recognize faces in images or video. In this system, dlib and OpenCV libraries are used for modeling and image processing tasks respectively.\n",
    "## Dlib\n",
    "\n",
    "Dlibis a modern C++ toolkit containing machine learning algorithms specilly computer vision and tools for creating complex software in C++ and python to solve real world problems. It is used in both industry and academia. \n",
    "\n",
    "[Dlib Official Sites](http://dlib.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc33a6b",
   "metadata": {},
   "source": [
    "## OpenCV \n",
    "OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products.\n",
    "\n",
    "[OpenCv Official Sites](https://opencv.org/about/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41fc28",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9367051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import face_recognition\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca78a9",
   "metadata": {},
   "source": [
    "## Generating Face Encodings with Dlib\n",
    "- To find face bounding boxes with dlib, we use the face_locations() function from the face_recognition library.     This function takes an image as input and returns the bounding boxes of all the faces present in the image.\n",
    "- The face_locations() function uses a dlib model (by default, the HOG model) to detect faces in the image.\n",
    "- To compute the facial embeddings or encodings with dlib, we use the face_encodings() function from the             face_recognition library. This function takes an image and a list of face bounding boxes as input, and returns a   list of 128-dimensional vectors representing the facial features of each face in the image.\n",
    "- The resulting encodings can be used for face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85260734",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images('training-dataset'))\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #Use Face_recognition to locate faces\n",
    "    boxes = face_recognition.face_locations(rgb,model='hog')\n",
    "    # compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    for encoding in encodings:\n",
    "        knownEncodings.append(encoding)\n",
    "        knownNames.append(name)\n",
    "#save emcodings along with their names in dictionary data\n",
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "f = open(\"face_enc\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f154e",
   "metadata": {},
   "source": [
    "## Face Recognition Using Dlib \n",
    "- Detecting the faces using the Haar cascades and storing the bounding box coordinates.\n",
    "- Generating the facial encodings for the detected faces using dlib face_encodings function.\n",
    "- Loading the known faces and encodings saved from system which we created.\n",
    "- Comparing the new generated user encodings with the  encodings in the data set using dlib compare_faces           function.\n",
    "- Drawing a rectangle around the detected face and displaying the predicted name on the output image than save and   show the ouput image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a86dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predicted : Steve Jobs\n"
     ]
    }
   ],
   "source": [
    "#find path of xml file containing haarcascade file\n",
    "cascPathface = os.path.dirname(\n",
    " cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "# load the harcaascade in the cascade classifier\n",
    "faceCascade = cv2.CascadeClassifier(cascPathface)\n",
    "# load the known faces and embeddings saved in last file\n",
    "data = pickle.loads(open('face_enc', \"rb\").read())\n",
    "# Input image path\n",
    "input_image_path = 'input-images/2.jpeg'\n",
    "image_name = input_image_path.split('/')\n",
    "output_image_path = 'output-images/predicted-'+image_name[1]\n",
    "image = cv2.imread(input_image_path)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#convert image to Greyscale for haarcascade\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(60, 60),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    " \n",
    "# the facial embeddings for face in input\n",
    "encodings = face_recognition.face_encodings(rgb)\n",
    "names = []\n",
    "# loop over the facial embeddings incase\n",
    "# we have multiple embeddings for multiple fcaes\n",
    "for encoding in encodings:\n",
    "    #Compare encodings with encodings in data[\"encodings\"]\n",
    "    #Matches contain array with boolean values and True for the embeddings it matches closely\n",
    "    #and False for rest\n",
    "    matches = face_recognition.compare_faces(data[\"encodings\"],\n",
    "    encoding)\n",
    "    name = \"Unknown\"\n",
    "    # check to see if we have found a match\n",
    "    if True in matches:\n",
    "        #Find positions at which we get True and store them\n",
    "        matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "        counts = {}\n",
    "        # loop over the matched indexes and maintain a count for\n",
    "        # each recognized face face\n",
    "        for i in matchedIdxs:\n",
    "            #Check the names at respective indexes we stored in matchedIdxs\n",
    "            name = data[\"names\"][i]\n",
    "            #increase count for the name we got\n",
    "            counts[name] = counts.get(name, 0) + 1\n",
    "            #set name which has highest count\n",
    "            name = max(counts, key=counts.get)\n",
    "        names.append(name)\n",
    "        print('Model Predicted :',names[0])\n",
    "        # loop over the recognized faces\n",
    "        for ((x, y, w, h), name) in zip(faces, names):\n",
    "            # rescale the face coordinates\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0,0, 255), 8)\n",
    "            cv2.putText(image, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "             0.75, (0, 255, 0), 4)\n",
    "    else :\n",
    "        names.append(name)\n",
    "        print(\"Model Predicted : \",names[0])\n",
    "\n",
    "        for ((x, y, w, h), name) in zip(faces, names):\n",
    "            # rescale the face coordinates\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0,0, 255), 8)\n",
    "            cv2.putText(image, name, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "             0.75, (0, 255, 0), 4)\n",
    "\n",
    "    cv2.imwrite(output_image_path+'-'+names[0]+'.png',image)\n",
    "    cv2.imshow(names[0],image)    \n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efa031",
   "metadata": {},
   "source": [
    "## Final Thoughts \n",
    "This simple facial recognition system uses the dlib library to detect faces in an input image, generates face encodings using pre-trained models, and matches these encodings with a pre-saved dataset of known faces to predict the identity of the input image. For more advanced and accurate facial recognition systems, one can train machine learning models such as KNN or CNN on large datasets to improve the accuracy of facial recognition.\n",
    "\n",
    "The CNN or KNN algorithm can achieve higher accuracy than the simple facial recognition system using dlib because it can learn more complex patterns and relationships between the face embeddings, and it can generalize better to new faces. However, it requires more computational resources and training data to achieve good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cad01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
